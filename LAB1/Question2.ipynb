{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "### Feedforward and Backpropagation Learning Algorithm for Multiple Perceptrons\n",
    "\n",
    "1. Initialize the weights and biases randomly.\n",
    "2. Implement the forward pass.\n",
    "3. Compute the loss between the predicted output and the actual output using an appropriate loss function.\n",
    "4. Compute the gradients of the loss function with respect to the weights and biases using the chain rule.\n",
    "5. Update the weights and biases.\n",
    "6. Iterate over multiple times (epochs), performing forward propagation, loss calculation, backpropagation, and parameter updates in each iteration till convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image.png' width=550>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3911335850337588\n",
      "Epoch 101, Loss: 0.04950035739299173\n",
      "Epoch 201, Loss: 0.017893115734211464\n",
      "Epoch 301, Loss: 0.010337741143537842\n",
      "Epoch 401, Loss: 0.007129433647996421\n",
      "Epoch 501, Loss: 0.005389486477082936\n",
      "Epoch 601, Loss: 0.004308621665512943\n",
      "Epoch 701, Loss: 0.0035764498843286066\n",
      "Epoch 801, Loss: 0.00304977234240024\n",
      "Epoch 901, Loss: 0.0026538222812070213\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self):\n",
    "        self.w_1 = np.random.randn(3, 3)  # 3 features -> 3 neurons in layer 1  (input layer to first layer)\n",
    "        self.b_1 = np.random.randn(3, 1)  # 3 neurons in layer 1\n",
    "        self.w_2 = np.random.randn(3, 2)  # 3 neurons in layer 1 -> 2 neurons in layer 2 (output)\n",
    "        self.b_2 = np.random.randn(2, 1)  # 2 neurons in output layer\n",
    "        \n",
    "        # Keeping input and target fixed\n",
    "        self.x = np.array([[1], [0.7], [1.2]])  # Input features (3x1)\n",
    "        self.t = np.array([[1], [0]])  # Target output (2x1)\n",
    "        self.learning_Rate = 0.1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        layer1_out = np.dot(self.w_1.T, self.x) + self.b_1\n",
    "        layer1_out = self.sigmoid(layer1_out)\n",
    "        layer2_out = np.dot(self.w_2.T, layer1_out) + self.b_2\n",
    "        layer2_out = self.sigmoid(layer2_out)\n",
    "\n",
    "        return layer1_out, layer2_out\n",
    "\n",
    "    def back_propagation(self, layer1_out, layer2_out):\n",
    "        # Calculate the error at the output layer (loss)\n",
    "        error = self.t - layer2_out\n",
    "        d_layer2_out = error * self.sigmoid_derivative(layer2_out)  # Derivative of the sigmoid function\n",
    "\n",
    "        # gradient for w_2 and b_2\n",
    "        d_w_2 = np.dot(layer1_out, d_layer2_out.T)  # (3x1) * (1x2) = (3x2)\n",
    "        d_b_2 = d_layer2_out  # Bias gradients are same as the error for the output layer (because derivative is 1)\n",
    "        \n",
    "        # Backpropagate the error to the first layer\n",
    "        d_layer1_out = np.dot(self.w_2, d_layer2_out) * self.sigmoid_derivative(layer1_out)\n",
    "\n",
    "        # Calculate the gradient for w_1 and b_1\n",
    "        d_w_1 = np.dot(self.x, d_layer1_out.T)  # (3x1) * (1x3) = (3x3)\n",
    "        d_b_1 = d_layer1_out  # Bias gradients are the same as the error for the first hidden layer\n",
    "\n",
    "        return d_w_1, d_b_1, d_w_2, d_b_2\n",
    "\n",
    "    def update_weights_and_biases(self, d_w_1, d_b_1, d_w_2, d_b_2):\n",
    "        self.w_1 += self.learning_Rate * d_w_1  \n",
    "        self.b_1 += self.learning_Rate * d_b_1  \n",
    "        self.w_2 += self.learning_Rate * d_w_2  \n",
    "        self.b_2 += self.learning_Rate * d_b_2  \n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            layer1_out, layer2_out = self.forward_propagation()\n",
    "\n",
    "            # Backpropagation\n",
    "            d_w_1, d_b_1, d_w_2, d_b_2 = self.back_propagation(layer1_out, layer2_out)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.update_weights_and_biases(d_w_1, d_b_1, d_w_2, d_b_2)\n",
    "\n",
    "            # Calculate loss (Mean Squared Error)\n",
    "            loss = np.mean((self.t - layer2_out) ** 2)\n",
    "            if epoch%100 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
    "\n",
    "# Create neural network instance\n",
    "nn = Neural_Network()\n",
    "\n",
    "# Train the neural network for 1000 epochs\n",
    "nn.train(epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preditions :  [1, 0]\n"
     ]
    }
   ],
   "source": [
    "preditions = nn.forward_propagation()\n",
    "output = [1,0] if preditions[1][0]>preditions[1][1] else [0,1]\n",
    "print(\"preditions : \",output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
